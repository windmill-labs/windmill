const __vite__mapDeps=(i,m=__vite__mapDeps,d=(m.f||(m.f=["assets/main-UcW-jkux.js","assets/index-DopNzxpa.js","assets/index-CJTkEeKe.css","assets/main-tDzW6kNL.js"])))=>i.map(i=>d[i]);
import{D as w,E as ie,S as re,b as G,T as oe,k as U,n as se,c as ae,d as ce,e as ge,o as ue,f as de,C as $,L as he,_ as A,g as le,R as me,h as L,i as pe,F as H,U as fe,t as _e,j as V,l,I as J,m as Y,p as K,q as Q,s as X,u as x,v as u,w as ke,x as Te,y as ye,z as D,A as Z,B as j,P as Se,G as be,H as Le,J as ve,K as we,M as Ce,N as W,O as ze,Q as Ee,V as Me,W as Ie,X as xe,Y as Pe,Z as De,$ as ee,a0 as v,a1 as je,a2 as Oe,a3 as Ae,a4 as Ne,a5 as Re,a6 as te,a7 as Fe,a8 as Be,a9 as Ve,aa as Ge}from"./index-DopNzxpa.js";class $e extends w{constructor(e,t,n,i,o,s,r){super(),this._grammar=e,this._initialState=t,this._containsEmbeddedLanguages=n,this._createBackgroundTokenizer=i,this._backgroundTokenizerShouldOnlyVerifyTokens=o,this._reportTokenizationTime=s,this._reportSlowTokenization=r,this._seenLanguages=[],this._onDidEncounterLanguage=this._register(new ie),this.onDidEncounterLanguage=this._onDidEncounterLanguage.event}get backgroundTokenizerShouldOnlyVerifyTokens(){return this._backgroundTokenizerShouldOnlyVerifyTokens()}getInitialState(){return this._initialState}tokenize(e,t,n){throw new Error("Not supported!")}createBackgroundTokenizer(e,t){if(this._createBackgroundTokenizer)return this._createBackgroundTokenizer(e,t)}tokenizeEncoded(e,t,n){const i=Math.random()*1e4<1,o=this._reportSlowTokenization||i,s=o?new re(!0):void 0,r=this._grammar.tokenizeLine2(e,n,500);if(o){const c=s.elapsed();(i||c>32)&&this._reportTokenizationTime(c,e.length,i)}if(r.stoppedEarly)return console.warn(`Time limit reached when tokenizing line: ${e.substring(0,100)}`),new G(r.tokens,n);if(this._containsEmbeddedLanguages){const c=this._seenLanguages,g=r.tokens;for(let h=0,m=g.length>>>1;h<m;h++){const P=g[(h<<1)+1],S=oe.getLanguageId(P);c[S]||(c[S]=!0,this._onDidEncounterLanguage.fire(S))}}let a;return n.equals(r.ruleStack)?a=n:a=r.ruleStack,new G(r.tokens,a)}}class We extends w{get backgroundTokenizerShouldOnlyVerifyTokens(){return this._actual.backgroundTokenizerShouldOnlyVerifyTokens}constructor(e,t,n,i){super(),this._encodedLanguageId=e,this._actual=t,this._maxTokenizationLineLength=i,this._register(U(this._maxTokenizationLineLength)),this._register(n)}getInitialState(){return this._actual.getInitialState()}tokenize(e,t,n){throw new Error("Not supported!")}tokenizeEncoded(e,t,n){return e.length>=this._maxTokenizationLineLength.get()?se(this._encodedLanguageId,n):this._actual.tokenizeEncoded(e,t,n)}createBackgroundTokenizer(e,t){if(this._actual.createBackgroundTokenizer)return this._actual.createBackgroundTokenizer(e,t)}}const b=class b{static getChannel(e){return e.getChannel(b.CHANNEL_NAME)}static setChannel(e,t){e.setChannel(b.CHANNEL_NAME,t)}};b.CHANNEL_NAME="textMateWorkerHost";let N=b;class qe{constructor(e){this.edits=e.slice().sort(ae(t=>t.offset,ce))}applyToArray(e){for(let t=this.edits.length-1;t>=0;t--){const n=this.edits[t];e.splice(n.offset,n.length,...new Array(n.newLength))}}}class Ue{constructor(e,t,n){this.offset=e,this.length=t,this.newLength=n}toString(){return`[${this.offset}, +${this.length}) -> +${this.newLength}}`}}class E{static fromMany(e){const t=e.map(n=>new E(n));return new He(t)}constructor(e){this.transformation=e,this.idx=0,this.offset=0}transform(e){let t=this.transformation.edits[this.idx];for(;t&&t.offset+t.length<=e;)this.offset+=t.newLength-t.length,this.idx++,t=this.transformation.edits[this.idx];if(!(t&&t.offset<=e))return e+this.offset}}class He{constructor(e){this.transformers=e}transform(e){for(const t of this.transformers){const n=t.transform(e);if(n===void 0)return;e=n}return e}}const I=class I extends w{constructor(e,t,n,i,o,s){super(),this._model=e,this._worker=t,this._languageIdCodec=n,this._backgroundTokenizationStore=i,this._configurationService=o,this._maxTokenizationLineLength=s,this.controllerId=I._id++,this._pendingChanges=[],this._states=new ge,this._loggingEnabled=ue("editor.experimental.asyncTokenizationLogging",!1,this._configurationService),this._register(U(this._loggingEnabled)),this._register(this._model.onDidChangeContent(c=>{this._shouldLog&&console.log("model change",{fileName:this._model.uri.fsPath.split("\\").pop(),changes:O(c.changes)}),this._worker.$acceptModelChanged(this.controllerId,c),this._pendingChanges.push(c)})),this._register(this._model.onDidChangeLanguage(c=>{const g=this._model.getLanguageId(),h=this._languageIdCodec.encodeLanguageId(g);this._worker.$acceptModelLanguageChanged(this.controllerId,g,h)}));const r=this._model.getLanguageId(),a=this._languageIdCodec.encodeLanguageId(r);this._worker.$acceptNewModel({uri:this._model.uri,versionId:this._model.getVersionId(),lines:this._model.getLinesContent(),EOL:this._model.getEOL(),languageId:r,encodedLanguageId:a,maxTokenizationLineLength:this._maxTokenizationLineLength.get(),controllerId:this.controllerId}),this._register(de(c=>{const g=this._maxTokenizationLineLength.read(c);this._worker.$acceptMaxTokenizationLineLength(this.controllerId,g)}))}dispose(){super.dispose(),this._worker.$acceptRemovedModel(this.controllerId)}requestTokens(e,t){this._worker.$retokenize(this.controllerId,e,t)}async setTokensAndStates(e,t,n,i){if(this.controllerId!==e)return;let o=$.deserialize(new Uint8Array(n));if(this._shouldLog&&console.log("received background tokenization result",{fileName:this._model.uri.fsPath.split("\\").pop(),updatedTokenLines:o.map(r=>r.getLineRange()).join(" & "),updatedStateLines:i.map(r=>new he(r.startLineNumber,r.startLineNumber+r.stateDeltas.length).toString()).join(" & ")}),this._shouldLog){const r=this._pendingChanges.filter(a=>a.versionId<=t).map(a=>a.changes).map(a=>O(a)).join(" then ");console.log("Applying changes to local states",r)}for(;this._pendingChanges.length>0&&this._pendingChanges[0].versionId<=t;){const r=this._pendingChanges.shift();this._states.acceptChanges(r.changes)}if(this._pendingChanges.length>0){if(this._shouldLog){const c=this._pendingChanges.map(g=>g.changes).map(g=>O(g)).join(" then ");console.log("Considering non-processed changes",c)}const r=E.fromMany(this._pendingChanges.map(c=>q(c.changes))),a=new $;for(const c of o)for(let g=c.startLineNumber;g<=c.endLineNumber;g++)r.transform(g-1)!==void 0&&a.add(g,c.getLineTokens(g));o=a.finalize();for(const c of this._pendingChanges)for(const g of c.changes)for(let h=0;h<o.length;h++)o[h].applyEdit(g.range,g.text)}const s=E.fromMany(this._pendingChanges.map(r=>q(r.changes)));if(!this._applyStateStackDiffFn||!this._initialState){const{applyStateStackDiff:r,INITIAL:a}=await A(async()=>{const{applyStateStackDiff:c,INITIAL:g}=await import("./main-UcW-jkux.js").then(h=>h.m);return{applyStateStackDiff:c,INITIAL:g}},__vite__mapDeps([0,1,2])).then(c=>c.default??c);this._applyStateStackDiffFn=r,this._initialState=a}for(const r of i){let a=r.startLineNumber<=1?this._initialState:this._states.getEndState(r.startLineNumber-1);for(let c=0;c<r.stateDeltas.length;c++){const g=r.stateDeltas[c];let h;g?(h=this._applyStateStackDiffFn(a,g),this._states.setEndState(r.startLineNumber+c,h)):h=this._states.getEndState(r.startLineNumber+c);const m=s.transform(r.startLineNumber+c-1);m!==void 0&&this._backgroundTokenizationStore.setEndState(m+1,h),r.startLineNumber+c>=this._model.getLineCount()-1&&this._backgroundTokenizationStore.backgroundTokenizationFinished(),a=h}}this._backgroundTokenizationStore.setTokens(o)}get _shouldLog(){return this._loggingEnabled.get()}};I._id=0;let R=I;function q(d){return new qe(d.map(e=>new Ue(e.range.startLineNumber-1,e.range.endLineNumber-e.range.startLineNumber+1,le(e.text)[0]+1)))}function O(d){return d.map(e=>me.lift(e.range).toString()+" => "+e.text).join(" & ")}var C,_;let F=(_=class{constructor(e,t,n,i,o,s,r){this._reportTokenizationTime=e,this._shouldTokenizeAsync=t,this._extensionResourceLoaderService=n,this._configurationService=i,this._languageService=o,this._notificationService=s,this._telemetryService=r,this._workerProxyPromise=null,this._worker=null,this._workerProxy=null,this._workerTokenizerControllers=new Map,this._currentTheme=null,this._currentTokenColorMap=null,this._grammarDefinitions=[]}dispose(){this._disposeWorker()}createBackgroundTokenizer(e,t,n){if(!this._shouldTokenizeAsync()||e.isTooLargeForSyncing())return;const i=new L,o=this._getWorkerProxy().then(s=>{if(i.isDisposed||!s)return;const r={controller:void 0,worker:this._worker};return i.add(Je(e,()=>{const a=new R(e,s,this._languageService.languageIdCodec,t,this._configurationService,n);return r.controller=a,this._workerTokenizerControllers.set(a.controllerId,a),_e(()=>{r.controller=void 0,this._workerTokenizerControllers.delete(a.controllerId),a.dispose()})})),r});return{dispose(){i.dispose()},requestTokens:async(s,r)=>{const a=await o;a!=null&&a.controller&&a.worker===this._worker&&a.controller.requestTokens(s,r)},reportMismatchingTokens:s=>{C._reportedMismatchingTokens||(C._reportedMismatchingTokens=!0,this._notificationService.error({message:"Async Tokenization Token Mismatch in line "+s,name:"Async Tokenization Token Mismatch"}),this._telemetryService.publicLog2("asyncTokenizationMismatchingTokens",{}))}}}setGrammarDefinitions(e){this._grammarDefinitions=e,this._disposeWorker()}acceptTheme(e,t){this._currentTheme=e,this._currentTokenColorMap=t,this._currentTheme&&this._currentTokenColorMap&&this._workerProxy&&this._workerProxy.$acceptTheme(this._currentTheme,this._currentTokenColorMap)}_getWorkerProxy(){return this._workerProxyPromise||(this._workerProxyPromise=this._createWorkerProxy()),this._workerProxyPromise}async _createWorkerProxy(){const e={grammarDefinitions:this._grammarDefinitions,onigurumaWASMUri:new URL("/ui_builder/assets/onig-Du5pRr7Y.wasm",import.meta.url).href},t=this._worker=pe(H.asBrowserUri("vs/workbench/services/textMate/browser/backgroundTokenization/worker/textMateTokenizationWorker.workerMain.js"),"TextMateWorker");return N.setChannel(t,{$readFile:async n=>{const i=fe.revive(n);return this._extensionResourceLoaderService.readExtensionResource(i)},$setTokensAndStates:async(n,i,o,s)=>{const r=this._workerTokenizerControllers.get(n);r&&r.setTokensAndStates(n,i,o,s)},$reportTokenizationTime:(n,i,o,s,r)=>{this._reportTokenizationTime(n,i,o,s,r)}}),await t.proxy.$init(e),this._worker!==t?null:(this._workerProxy=t.proxy,this._currentTheme&&this._currentTokenColorMap&&this._workerProxy.$acceptTheme(this._currentTheme,this._currentTokenColorMap),t.proxy)}_disposeWorker(){for(const e of this._workerTokenizerControllers.values())e.dispose();this._workerTokenizerControllers.clear(),this._worker&&(this._worker.dispose(),this._worker=null),this._workerProxy=null,this._workerProxyPromise=null}},C=_,_._reportedMismatchingTokens=!1,_);F=C=V([l(2,J),l(3,Y),l(4,K),l(5,Q),l(6,X)],F);function Je(d,e){const t=new L,n=t.add(new L);function i(){d.isAttachedToEditor()?n.add(e()):n.clear()}return i(),t.add(d.onDidChangeAttached(()=>{i()})),t}class Ye{constructor(){this._scopeNameToLanguageRegistration=Object.create(null)}reset(){this._scopeNameToLanguageRegistration=Object.create(null)}register(e){this._scopeNameToLanguageRegistration[e.scopeName]=e}getGrammarDefinition(e){return this._scopeNameToLanguageRegistration[e]||null}}const z="No TM Grammar registered for this language.";class Ke extends w{constructor(e,t,n,i){super(),this._host=e,this._initialState=n.INITIAL,this._scopeRegistry=new Ye,this._injections={},this._injectedEmbeddedLanguages={},this._languageToScope=new Map,this._grammarRegistry=this._register(new n.Registry({onigLib:i,loadGrammar:async o=>{const s=this._scopeRegistry.getGrammarDefinition(o);if(!s)return this._host.logTrace(`No grammar found for scope ${o}`),null;const r=s.location;try{const a=await this._host.readFile(r);return n.parseRawGrammar(a,r.path)}catch(a){return this._host.logError(`Unable to load and parse grammar for scope ${o} from ${r}`,a),null}},getInjections:o=>{const s=o.split(".");let r=[];for(let a=1;a<=s.length;a++){const c=s.slice(0,a).join(".");r=[...r,...this._injections[c]||[]]}return r}}));for(const o of t){if(this._scopeRegistry.register(o),o.injectTo){for(const s of o.injectTo){let r=this._injections[s];r||(this._injections[s]=r=[]),r.push(o.scopeName)}if(o.embeddedLanguages)for(const s of o.injectTo){let r=this._injectedEmbeddedLanguages[s];r||(this._injectedEmbeddedLanguages[s]=r=[]),r.push(o.embeddedLanguages)}}o.language&&this._languageToScope.set(o.language,o.scopeName)}}has(e){return this._languageToScope.has(e)}setTheme(e,t){this._grammarRegistry.setTheme(e,t)}getColorMap(){return this._grammarRegistry.getColorMap()}async createGrammar(e,t){const n=this._languageToScope.get(e);if(typeof n!="string")throw new Error(z);const i=this._scopeRegistry.getGrammarDefinition(n);if(!i)throw new Error(z);const o=i.embeddedLanguages;if(this._injectedEmbeddedLanguages[n]){const a=this._injectedEmbeddedLanguages[n];for(const c of a)for(const g of Object.keys(c))o[g]=c[g]}const s=Object.keys(o).length>0;let r;try{r=await this._grammarRegistry.loadGrammarWithConfiguration(n,t,{embeddedLanguages:o,tokenTypes:i.tokenTypes,balancedBracketSelectors:i.balancedBracketSelectors,unbalancedBracketSelectors:i.unbalancedBracketSelectors})}catch(a){throw a.message&&a.message.startsWith("No grammar provided for")?new Error(z):a}return{languageId:e,grammar:r,initialState:this._initialState,containsEmbeddedLanguages:s,sourceExtensionId:i.sourceExtensionId}}}const f=x.registerExtensionPoint({extensionPoint:"grammars",deps:[ke],jsonSchema:{description:u(12450,"Contributes textmate tokenizers."),type:"array",defaultSnippets:[{body:[{language:"${1:id}",scopeName:"source.${2:id}",path:"./syntaxes/${3:id}.tmLanguage."}]}],items:{type:"object",defaultSnippets:[{body:{language:"${1:id}",scopeName:"source.${2:id}",path:"./syntaxes/${3:id}.tmLanguage."}}],properties:{language:{description:u(12451,"Language identifier for which this syntax is contributed to."),type:"string"},scopeName:{description:u(12452,"Textmate scope name used by the tmLanguage file."),type:"string"},path:{description:u(12453,"Path of the tmLanguage file. The path is relative to the extension folder and typically starts with './syntaxes/'."),type:"string"},embeddedLanguages:{description:u(12454,"A map of scope name to language id if this grammar contains embedded languages."),type:"object"},tokenTypes:{description:u(12455,"A map of scope name to token types."),type:"object",additionalProperties:{enum:["string","comment","other"]}},injectTo:{description:u(12456,"List of language scope names to which this grammar is injected to."),type:"array",items:{type:"string"}},balancedBracketScopes:{description:u(12457,"Defines which scope names contain balanced brackets."),type:"array",items:{type:"string"},default:["*"]},unbalancedBracketScopes:{description:u(12458,"Defines which scope names do not contain balanced brackets."),type:"array",items:{type:"string"},default:[]}},required:["scopeName","path"]}}});var T,k;let B=(k=class extends w{constructor(e,t,n,i,o,s,r,a,c,g){super(),this._languageService=e,this._themeService=t,this._extensionResourceLoaderService=n,this._notificationService=i,this._logService=o,this._configurationService=s,this._progressService=r,this._environmentService=a,this._instantiationService=c,this._telemetryService=g,this._createdModes=[],this._encounteredLanguages=[],this._debugMode=!1,this._debugModePrintFunc=()=>{},this._grammarDefinitions=null,this._grammarFactory=null,this._tokenizersRegistrations=this._register(new L),this._currentTheme=null,this._currentTokenColorMap=null,this._threadedBackgroundTokenizerFactory=this._instantiationService.createInstance(F,(h,m,P,S,ne)=>this._reportTokenizationTime(h,m,P,S,!0,ne),()=>this.getAsyncTokenizationEnabled()),this._vscodeOniguruma=null,this._styleElement=Te(),this._styleElement.className="vscode-tokens-styles",f.setHandler(h=>this._handleGrammarsExtPoint(h)),this._updateTheme(this._themeService.getColorTheme(),!0),this._register(this._themeService.onDidColorThemeChange(()=>{this._updateTheme(this._themeService.getColorTheme(),!1)})),this._register(this._languageService.onDidRequestRichLanguageFeatures(h=>{this._createdModes.push(h)}))}getAsyncTokenizationEnabled(){return!!this._configurationService.getValue("editor.experimental.asyncTokenization")}getAsyncTokenizationVerification(){return!!this._configurationService.getValue("editor.experimental.asyncTokenizationVerification")}_handleGrammarsExtPoint(e){this._grammarDefinitions=null,this._grammarFactory&&(this._grammarFactory.dispose(),this._grammarFactory=null),this._tokenizersRegistrations.clear(),this._grammarDefinitions=[];for(const t of e){const n=t.value;for(const i of n){const o=this._validateGrammarDefinition(t,i);if(o&&(this._grammarDefinitions.push(o),o.language)){const s=new ye(()=>this._createTokenizationSupport(o.language));this._tokenizersRegistrations.add(s),this._tokenizersRegistrations.add(D.registerFactory(o.language,s))}}}this._threadedBackgroundTokenizerFactory.setGrammarDefinitions(this._grammarDefinitions);for(const t of this._createdModes)D.getOrCreate(t)}_validateGrammarDefinition(e,t){if(!Ze(e.description.extensionLocation,t,e.collector,this._languageService))return null;const n=Z(e.description.extensionLocation,t.path),i=Object.create(null);if(t.embeddedLanguages){const a=Object.keys(t.embeddedLanguages);for(let c=0,g=a.length;c<g;c++){const h=a[c],m=t.embeddedLanguages[h];typeof m=="string"&&this._languageService.isRegisteredLanguageId(m)&&(i[h]=this._languageService.languageIdCodec.encodeLanguageId(m))}}const o=Object.create(null);if(t.tokenTypes){const a=Object.keys(t.tokenTypes);for(const c of a)switch(t.tokenTypes[c]){case"string":o[c]=j.String;break;case"other":o[c]=j.Other;break;case"comment":o[c]=j.Comment;break}}const s=t.language&&this._languageService.isRegisteredLanguageId(t.language)?t.language:void 0;function r(a,c){return!Array.isArray(a)||!a.every(g=>typeof g=="string")?c:a}return{location:n,language:s,scopeName:t.scopeName,embeddedLanguages:i,tokenTypes:o,injectTo:t.injectTo,balancedBracketSelectors:r(t.balancedBracketScopes,["*"]),unbalancedBracketSelectors:r(t.unbalancedBracketScopes,[]),sourceExtensionId:e.description.id}}startDebugMode(e,t){if(this._debugMode){this._notificationService.error(u(12439,"Already Logging."));return}this._debugModePrintFunc=e,this._debugMode=!0,this._debugMode&&this._progressService.withProgress({location:Se.Notification,buttons:[u(12440,"Stop")]},n=>(n.report({message:u(12441,"Preparing to log TM Grammar parsing. Press Stop when finished.")}),this._getVSCodeOniguruma().then(i=>(i.setDefaultDebugCall(!0),n.report({message:u(12442,"Now logging TM Grammar parsing. Press Stop when finished.")}),new Promise((o,s)=>{})))),n=>{this._getVSCodeOniguruma().then(i=>{this._debugModePrintFunc=()=>{},this._debugMode=!1,i.setDefaultDebugCall(!1),t()})})}_canCreateGrammarFactory(){return!!this._grammarDefinitions}async _getOrCreateGrammarFactory(){if(this._grammarFactory)return this._grammarFactory;const[e,t]=await Promise.all([A(()=>import("./main-UcW-jkux.js").then(i=>i.m),__vite__mapDeps([0,1,2])).then(i=>i.default??i),this._getVSCodeOniguruma()]),n=Promise.resolve({createOnigScanner:i=>t.createOnigScanner(i),createOnigString:i=>t.createOnigString(i)});return this._grammarFactory?this._grammarFactory:(this._grammarFactory=new Ke({logTrace:i=>this._logService.trace(i),logError:(i,o)=>this._logService.error(i,o),readFile:i=>this._extensionResourceLoaderService.readExtensionResource(i)},this._grammarDefinitions||[],e,n),this._updateTheme(this._themeService.getColorTheme(),!0),this._grammarFactory)}async _createTokenizationSupport(e){if(!this._languageService.isRegisteredLanguageId(e)||!this._canCreateGrammarFactory())return null;try{const t=await this._getOrCreateGrammarFactory();if(!t.has(e))return null;const n=this._languageService.languageIdCodec.encodeLanguageId(e),i=await t.createGrammar(e,n);if(!i.grammar)return null;const o=et("editor.maxTokenizationLineLength",e,-1,this._configurationService),s=new L,r=s.add(new $e(i.grammar,i.initialState,i.containsEmbeddedLanguages,(a,c)=>this._threadedBackgroundTokenizerFactory.createBackgroundTokenizer(a,c,o),()=>this.getAsyncTokenizationVerification(),(a,c,g)=>{this._reportTokenizationTime(a,e,i.sourceExtensionId,c,!1,g)},!0));return s.add(r.onDidEncounterLanguage(a=>{if(!this._encounteredLanguages[a]){const c=this._languageService.languageIdCodec.decodeLanguageId(a);this._encounteredLanguages[a]=!0,this._languageService.requestBasicLanguageFeatures(c)}})),new We(n,r,s,o)}catch(t){return t.message&&t.message===z||be(t),null}}_updateTheme(e,t){var o;if(!t&&this._currentTheme&&this._currentTokenColorMap&&Xe(this._currentTheme.settings,e.tokenColors)&&Le(this._currentTokenColorMap,e.tokenColorMap))return;this._currentTheme={name:e.label,settings:e.tokenColors},this._currentTokenColorMap=e.tokenColorMap,(o=this._grammarFactory)==null||o.setTheme(this._currentTheme,this._currentTokenColorMap);const n=Qe(this._currentTokenColorMap),i=ve(n);this._styleElement.textContent=i,D.setColorMap(n),this._currentTheme&&this._currentTokenColorMap&&this._threadedBackgroundTokenizerFactory.acceptTheme(this._currentTheme,this._currentTokenColorMap)}async createTokenizer(e){if(!this._languageService.isRegisteredLanguageId(e))return null;const t=await this._getOrCreateGrammarFactory();if(!t.has(e))return null;const n=this._languageService.languageIdCodec.encodeLanguageId(e),{grammar:i}=await t.createGrammar(e,n);return i}_getVSCodeOniguruma(){return this._vscodeOniguruma||(this._vscodeOniguruma=(async()=>{const[e,t]=await Promise.all([A(()=>import("./main-tDzW6kNL.js").then(n=>n.m),__vite__mapDeps([3,1,2])).then(n=>n.default??n),this._loadVSCodeOnigurumaWASM()]);return await e.loadWASM({data:t,print:n=>{this._debugModePrintFunc(n)}}),e})()),this._vscodeOniguruma}async _loadVSCodeOnigurumaWASM(){return we?await(await fetch(new URL("/ui_builder/assets/onig-Du5pRr7Y.wasm",import.meta.url).href)).arrayBuffer():await fetch(H.asBrowserUri(`${Ce}/vscode-oniguruma/release/onig.wasm`).toString(!0))}_reportTokenizationTime(e,t,n,i,o,s){const r=o?"async":"sync";T.reportTokenizationTimeCounter[r]>50||(T.reportTokenizationTimeCounter[r]===0&&setTimeout(()=>{T.reportTokenizationTimeCounter[r]=0},1e3*60*60),T.reportTokenizationTimeCounter[r]++,this._telemetryService.publicLog2("editor.tokenizedLine",{timeMs:e,languageId:t,lineLength:i,fromWorker:o,sourceExtensionId:n,isRandomSample:s,tokenizationSetting:this.getAsyncTokenizationEnabled()?this.getAsyncTokenizationVerification()?2:1:0}))}},T=k,k.reportTokenizationTimeCounter={sync:0,async:0},k);B=T=V([l(0,K),l(1,Ie),l(2,J),l(3,Q),l(4,xe),l(5,Y),l(6,Pe),l(7,De),l(8,ee),l(9,X)],B);function Qe(d){const e=[null];for(let t=1,n=d.length;t<n;t++)e[t]=Me.fromHex(d[t]);return e}function Xe(d,e){if(!e||!d||e.length!==d.length)return!1;for(let t=e.length-1;t>=0;t--){const n=e[t],i=d[t];if(n.scope!==i.scope)return!1;const o=n.settings,s=i.settings;if(o&&s){if(o.fontStyle!==s.fontStyle||o.foreground!==s.foreground||o.background!==s.background)return!1}else if(!o||!s)return!1}return!0}function Ze(d,e,t,n){if(e.language&&(typeof e.language!="string"||!n.isRegisteredLanguageId(e.language)))return t.error(u(12443,"Unknown language in `contributes.{0}.language`. Provided value: {1}",f.name,String(e.language))),!1;if(!e.scopeName||typeof e.scopeName!="string")return t.error(u(12444,"Expected string in `contributes.{0}.scopeName`. Provided value: {1}",f.name,String(e.scopeName))),!1;if(!e.path||typeof e.path!="string")return t.error(u(12445,"Expected string in `contributes.{0}.path`. Provided value: {1}",f.name,String(e.path))),!1;if(e.injectTo&&(!Array.isArray(e.injectTo)||e.injectTo.some(o=>typeof o!="string")))return t.error(u(12446,"Invalid value in `contributes.{0}.injectTo`. Must be an array of language scope names. Provided value: {1}",f.name,JSON.stringify(e.injectTo))),!1;if(e.embeddedLanguages&&!W(e.embeddedLanguages))return t.error(u(12447,"Invalid value in `contributes.{0}.embeddedLanguages`. Must be an object map from scope name to language. Provided value: {1}",f.name,JSON.stringify(e.embeddedLanguages))),!1;if(e.tokenTypes&&!W(e.tokenTypes))return t.error(u(12448,"Invalid value in `contributes.{0}.tokenTypes`. Must be an object map from scope name to token type. Provided value: {1}",f.name,JSON.stringify(e.tokenTypes))),!1;const i=Z(d,e.path);return ze(i,d)||t.warn(u(12449,"Expected `contributes.{0}.path` ({1}) to be included inside extension's folder ({2}). This might make the extension non-portable.",f.name,i.path,d.path)),!0}function et(d,e,t,n){return Ee(i=>n.onDidChangeConfiguration(o=>{o.affectsConfiguration(d,{overrideIdentifier:e})&&i(o)}),()=>n.getValue(d,{overrideIdentifier:e})??t)}const p=Ae(),tt=x.registerExtensionPoint({extensionPoint:"semanticTokenTypes",jsonSchema:{description:u(12646,"Contributes semantic token types."),type:"array",items:{type:"object",properties:{id:{type:"string",description:u(12647,"The identifier of the semantic token type"),pattern:v,patternErrorMessage:u(12648,"Identifiers should be in the form letterOrDigit[_-letterOrDigit]*")},superType:{type:"string",description:u(12649,"The super type of the semantic token type"),pattern:v,patternErrorMessage:u(12650,"Super types should be in the form letterOrDigit[_-letterOrDigit]*")},description:{type:"string",description:u(12651,"The description of the semantic token type")}}}}}),nt=x.registerExtensionPoint({extensionPoint:"semanticTokenModifiers",jsonSchema:{description:u(12652,"Contributes semantic token modifiers."),type:"array",items:{type:"object",properties:{id:{type:"string",description:u(12653,"The identifier of the semantic token modifier"),pattern:v,patternErrorMessage:u(12654,"Identifiers should be in the form letterOrDigit[_-letterOrDigit]*")},description:{type:"string",description:u(12655,"The description of the semantic token modifier")}}}}}),it=x.registerExtensionPoint({extensionPoint:"semanticTokenScopes",jsonSchema:{description:u(12656,"Contributes semantic token scope maps."),type:"array",items:{type:"object",properties:{language:{description:u(12657,"Lists the languge for which the defaults are."),type:"string"},scopes:{description:u(12658,"Maps a semantic token (described by semantic token selector) to one or more textMate scopes used to represent that token."),type:"object",additionalProperties:{type:"array",items:{type:"string"}}}}}}});class rt{constructor(){function e(t,n,i){if(typeof t.id!="string"||t.id.length===0)return i.error(u(12659,"'configuration.{0}.id' must be defined and can not be empty",n)),!1;if(!t.id.match(v))return i.error(u(12660,"'configuration.{0}.id' must follow the pattern letterOrDigit[-_letterOrDigit]*",n)),!1;const o=t.superType;return o&&!o.match(v)?(i.error(u(12661,"'configuration.{0}.superType' must follow the pattern letterOrDigit[-_letterOrDigit]*",n)),!1):typeof t.description!="string"||t.id.length===0?(i.error(u(12662,"'configuration.{0}.description' must be defined and can not be empty",n)),!1):!0}tt.setHandler((t,n)=>{for(const i of n.added){const o=i.value,s=i.collector;if(!o||!Array.isArray(o)){s.error(u(12663,"'configuration.semanticTokenType' must be an array"));return}for(const r of o)e(r,"semanticTokenType",s)&&p.registerTokenType(r.id,r.description,r.superType)}for(const i of n.removed){const o=i.value;for(const s of o)p.deregisterTokenType(s.id)}}),nt.setHandler((t,n)=>{for(const i of n.added){const o=i.value,s=i.collector;if(!o||!Array.isArray(o)){s.error(u(12664,"'configuration.semanticTokenModifier' must be an array"));return}for(const r of o)e(r,"semanticTokenModifier",s)&&p.registerTokenModifier(r.id,r.description)}for(const i of n.removed){const o=i.value;for(const s of o)p.deregisterTokenModifier(s.id)}}),it.setHandler((t,n)=>{for(const i of n.added){const o=i.value,s=i.collector;if(!o||!Array.isArray(o)){s.error(u(12665,"'configuration.semanticTokenScopes' must be an array"));return}for(const r of o){if(r.language&&typeof r.language!="string"){s.error(u(12666,"'configuration.semanticTokenScopes.language' must be a string"));continue}if(!r.scopes||typeof r.scopes!="object"){s.error(u(12667,"'configuration.semanticTokenScopes.scopes' must be defined as an object"));continue}for(const a in r.scopes){const c=r.scopes[a];if(!Array.isArray(c)||c.some(g=>typeof g!="string")){s.error(u(12668,"'configuration.semanticTokenScopes.scopes' values must be an array of strings"));continue}try{const g=p.parseTokenSelector(a,r.language);p.registerTokenStyleDefault(g,{scopesToProbe:c.map(h=>h.split(" "))})}catch{s.error(u(12669,"configuration.semanticTokenScopes.scopes': Problems parsing selector {0}.",a))}}}}for(const i of n.removed){const o=i.value;for(const s of o)for(const r in s.scopes){const a=s.scopes[r];try{const c=p.parseTokenSelector(r,s.language);p.registerTokenStyleDefault(c,{scopesToProbe:a.map(g=>g.split(" "))})}catch{}}}})}}var y;let M=(y=class{constructor(e){this.instantiationService=e,this.instantiationService.createInstance(rt)}},y.ID="workbench.contrib.tokenClassificationExtensionPoint",y);M=V([l(0,ee)],M);je(M.ID,M,Oe.BlockStartup);Fe(async d=>{d.get(Be).when(Ve.Ready).then(()=>{Ge.get(te)})});function st(){return{...Re(),[te.toString()]:new Ne(B,[],!1)}}export{te as ITextMateTokenizationService,st as default};
