version: "3.7"

services:
  db:
    deploy:
      # To use an external database, set replicas to 0 and set DATABASE_URL to the external database url in the .env file
      replicas: 1
    image: postgres:14
    restart: unless-stopped
    volumes:
      - db_data:/var/lib/postgresql/data
    expose:
      - 5432
    environment:
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: windmill
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  windmill_server:
    image: ${WM_IMAGE}
    pull_policy: always
    deploy:
      replicas: 1
    restart: unless-stopped
    expose:
      - 8000
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - BASE_URL=${WM_BASE_URL}
      - RUST_LOG=info
      ## You can set the number of workers to 1 and not need any separate worker service but not recommended
      - NUM_WORKERS=0
      - DISABLE_SERVER=false
      - METRICS_ADDR=false
      - REQUEST_SIZE_LIMIT=${WM_REQUEST_SIZE_LIMIT}
      # LICENSE_KEY is only needed for the enterprise edition
      - LICENSE_KEY=${WM_LICENSE_KEY}
    depends_on:
      db:
        condition: service_healthy
    volumes:
      # See Oauth (https://docs.windmill.dev/docs/misc/setup_oauth)
      - ./oauth.json:/usr/src/app/oauth.json

  windmill_worker:
    image: ${WM_IMAGE}
    pull_policy: always
    deploy:
      replicas: 3
    restart: unless-stopped
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - BASE_URL=${WM_BASE_URL}
      - RUST_LOG=info
      - DISABLE_SERVER=true
      - KEEP_JOB_DIR=false
      - METRICS_ADDR=false
      # To handle all tags, remove the env variable altogether. If you do so, you can remove the windmill_worker_native containers.
      - WORKER_TAGS=deno,python3,go,bash,powershell,dependency,flow,hub,other,bun
      # LICENSE_KEY is only needed for the enterprise edition
      - LICENSE_KEY=${WM_LICENSE_KEY}
    depends_on:
      db:
        condition: service_healthy
    # to mount the worker folder to debug, KEEP_JOB_DIR=true and mount /tmp/windmill
    volumes:
      # mount the docker socket to allow to run docker containers from within the workers
      - /var/run/docker.sock:/var/run/docker.sock
      # See Oauth (https://docs.windmill.dev/docs/misc/setup_oauth)
      - ./oauth.json:/usr/src/app/oauth.json
      - worker_dependency_cache:/tmp/windmill/cache

  ## This worker is specialized for "native" jobs. They run in-process and can thus be parallelized to more than 1 at a time on a given worker which is why NUM_WORKERS is set to 4
  windmill_worker_native:
    # Use ghcr.io/windmill-labs/windmill-ee:main for the ee
    image: ${WM_IMAGE}
    pull_policy: always
    deploy:
      replicas: 1
      resources:
          limits:
            cpus: "0.25"
            memory: 512M
    restart: unless-stopped
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - BASE_URL=${WM_BASE_URL}
      - RUST_LOG=info
      - DISABLE_SERVER=true
      - KEEP_JOB_DIR=false
      - METRICS_ADDR=false
      - NUM_WORKERS=4
      - WORKER_TAGS=nativets,postgresql,mysql,graphql,snowflake
      # LICENSE_KEY is only needed for the enterprise edition
      # - LICENSE_KEY=${WM_LICENSE_KEY}
    depends_on:
      db:
        condition: service_healthy
    volumes:
      # See Oauth (https://docs.windmill.dev/docs/misc/setup_oauth)
      - ./oauth.json:/usr/src/app/oauth.json


  lsp:
    image: ghcr.io/windmill-labs/windmill-lsp:latest
    restart: unless-stopped
    expose:
      - 3001
    volumes:
      - lsp_cache:/root/.cache

  multiplayer:
    image: ghcr.io/windmill-labs/windmill-multiplayer:latest
    deploy:
      replicas: 0 # Set to 1 to enable multiplayer, only available on Enterprise Edition
    restart: unless-stopped
    expose:
      - 3002

  caddy:
    image: caddy:2.5.2-alpine
    restart: unless-stopped

    # Configure the mounted Caddyfile and the exposed ports or use another reverse proxy if needed
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
    ports:
      # To change the exposed port, simply change 80:80 to <desired_port>:80. No other changes needed
      - 80:80
      #- 443:443 # Uncomment to enable HTTPS handling by Caddy
    environment:
      - BASE_URL=":80"
      #- BASE_URL=mydomain.com # Uncomment and comment line above to enable HTTPS handling by Caddy

volumes:
  db_data: null
  worker_dependency_cache: null
  lsp_cache: null
